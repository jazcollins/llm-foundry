{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall -y pyarrow\n",
    "# ! pip install pyarrow==9.0.0\n",
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/mnt/workdisk/jasmine/LLaVA\")\n",
    "from llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava import conversation as conversation_lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_multimodal(\n",
    "    sources: Sequence[str],\n",
    "    data_args: DataArguments\n",
    ") -> Dict:\n",
    "    is_multimodal = data_args.is_multimodal\n",
    "    if not is_multimodal:\n",
    "        return sources\n",
    "\n",
    "    for source in sources:\n",
    "        for sentence in source:\n",
    "            if DEFAULT_IMAGE_TOKEN in sentence['value']:\n",
    "                sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "                sentence['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + sentence['value']\n",
    "                sentence['value'] = sentence['value'].strip()\n",
    "                if \"mmtag\" in conversation_lib.default_conversation.version:\n",
    "                    sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '<Image>' + DEFAULT_IMAGE_TOKEN + '</Image>')\n",
    "            replace_token = DEFAULT_IMAGE_TOKEN\n",
    "            if data_args.mm_use_im_start_end:\n",
    "                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "            sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "\n",
    "    return sources\n",
    "\n",
    "def preprocess_mpt(\n",
    "    sources,\n",
    "    # tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv = conversation_lib.default_conversation.copy()\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "    input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\n",
    "    targets = input_ids.clone()\n",
    "    assert conv.sep_style == conversation_lib.SeparatorStyle.MPT\n",
    "\n",
    "    # Mask targets\n",
    "    sep = conv.sep + conv.roles[1]\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        rounds = conversation.split(conv.sep)\n",
    "        re_rounds = [conv.sep.join(rounds[:3])] # system + user + gpt\n",
    "        for conv_idx in range(3, len(rounds), 2):\n",
    "            re_rounds.append(conv.sep.join(rounds[conv_idx:conv_idx+2]))    # user + gpt\n",
    "        cur_len = 0\n",
    "        target[:cur_len] = IGNORE_INDEX\n",
    "        for i, rou in enumerate(re_rounds):\n",
    "            if rou == \"\":\n",
    "                break\n",
    "\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            round_len = len(tokenizer_image_token(rou, tokenizer)) + len(tokenizer_image_token(conv.sep, tokenizer))\n",
    "            instruction_len = len(tokenizer_image_token(parts[0], tokenizer))\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "\n",
    "            cur_len += round_len\n",
    "        target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_INDEX\n",
    "                print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "    is_multimodal: bool = False\n",
    "    image_folder: Optional[str] = field(default=None)\n",
    "    image_aspect_ratio: str = 'square'\n",
    "    \n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                #  tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_args: DataArguments):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "\n",
    "        print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.list_data_dict = list_data_dict\n",
    "        self.data_args = data_args\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    @property\n",
    "    def lengths(self):\n",
    "        length_list = []\n",
    "        for sample in self.list_data_dict:\n",
    "            img_tokens = 128 if 'image' in sample else 0\n",
    "            length_list.append(sum(len(conv['value'].split()) for conv in sample['conversations']) + img_tokens)\n",
    "        return length_list\n",
    "\n",
    "    @property\n",
    "    def modality_lengths(self):\n",
    "        length_list = []\n",
    "        for sample in self.list_data_dict:\n",
    "            cur_len = sum(len(conv['value'].split()) for conv in sample['conversations'])\n",
    "            cur_len = cur_len if 'image' in sample else -cur_len\n",
    "            length_list.append(cur_len)\n",
    "        return length_list\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.list_data_dict[i]\n",
    "        if isinstance(i, int):\n",
    "            sources = [sources]\n",
    "        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n",
    "        if 'image' in sources[0]:\n",
    "            image_file = self.list_data_dict[i]['image']\n",
    "            image_folder = self.data_args.image_folder\n",
    "            # processor = self.data_args.image_processor\n",
    "            image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\n",
    "            if self.data_args.image_aspect_ratio == 'pad':\n",
    "                def expand2square(pil_img, background_color):\n",
    "                    width, height = pil_img.size\n",
    "                    if width == height:\n",
    "                        return pil_img\n",
    "                    elif width > height:\n",
    "                        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "                        result.paste(pil_img, (0, (width - height) // 2))\n",
    "                        return result\n",
    "                    else:\n",
    "                        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "                        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "                        return result\n",
    "                # image = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n",
    "                image = expand2square(image, (255, 255, 255))\n",
    "                # image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "            else:\n",
    "                pass\n",
    "                # image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "            sources = preprocess_multimodal(\n",
    "                copy.deepcopy([e[\"conversations\"] for e in sources]),\n",
    "                self.data_args)\n",
    "        else:\n",
    "            sources = copy.deepcopy([e[\"conversations\"] for e in sources])\n",
    "\n",
    "        # data_dict = preprocess(\n",
    "        #     sources,\n",
    "        #     self.tokenizer,\n",
    "        #     has_image=('image' in self.list_data_dict[i]))\n",
    "\n",
    "        data_dict = preprocess_mpt(sources)\n",
    "        \n",
    "        if isinstance(i, int):\n",
    "            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n",
    "                             labels=data_dict[\"labels\"][0])\n",
    "\n",
    "        data_dict['sources'] = sources\n",
    "\n",
    "        # image exist in the data\n",
    "        if 'image' in self.list_data_dict[i]:\n",
    "            data_dict['image'] = image\n",
    "        elif self.data_args.is_multimodal:\n",
    "            # image does not exist in the data, but the model is multimodal\n",
    "            crop_size = 128 #self.data_args.image_processor.crop_size\n",
    "            data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/mnt/workdisk/jasmine/data/llava/LLaVA-Instruct-150K/v1-ft/llava_instruct_80k.json\"\n",
    "image_folder = '/mnt/workdisk/jasmine/data/llava/data/train2017'\n",
    "data_args = DataArguments(data_path = data_path, image_folder = image_folder, image_aspect_ratio='pad')\n",
    "\n",
    "\n",
    "dataset = LazySupervisedDataset(data_path, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# dataset = load_dataset(\"liuhaotian/LLaVA-CC3M-Pretrain-595K\")\n",
    "# dataset = load_dataset(\"/mnt/workdisk/jasmine/data/llava/LLaVA-Instruct-150K/tmp\")\n",
    "dataset = load_dataset(\"/mnt/workdisk/jasmine/data/llava/LLaVA-Instruct-150K/v1-ft\")\n",
    "\n",
    "# Print the first example\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(50): #len(dataset)):\n",
    "\n",
    "#     # image = os.path.join(image_folder, dataset['train'][i]['image'])\n",
    "#     # img = Image.open(image)\n",
    "#     # img.show()\n",
    "#     convos = dataset[\"train\"][i]['conversations']\n",
    "#     print(len(convos))\n",
    "#     # print(convos[0])\n",
    "#     # print(convos[1])\n",
    "#     # print(len(convos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    image = os.path.join(image_folder, dataset['train'][i]['image'])\n",
    "    img = Image.open(image)\n",
    "    img.show()\n",
    "    convos = dataset[\"train\"][i]['conversations']\n",
    "    for convo in convos:\n",
    "        print(convo)\n",
    "\n",
    "    print(len(convos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv = conversation_lib.default_conversation.copy()\n",
    "# roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "# # print(roles)\n",
    "\n",
    "# sources = [dataset[\"train\"][0]['conversations'], dataset[\"train\"][1]['conversations'], dataset[\"train\"][2]['conversations']]\n",
    "# # print(sources)\n",
    "\n",
    "# # Apply prompt templates\n",
    "# conversations = []\n",
    "# for i, source in enumerate(sources):\n",
    "#     if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "#         # Skip the first one if it is not from human\n",
    "#         source = source[1:]\n",
    "\n",
    "#     conv.messages = []\n",
    "#     for j, sentence in enumerate(source):\n",
    "#         role = roles[sentence[\"from\"]]\n",
    "#         assert role == conv.roles[j % 2], f\"{i}\"\n",
    "#         conv.append_message(role, sentence[\"value\"])\n",
    "#     conversations.append(conv.get_prompt())\n",
    "\n",
    "\n",
    "# for convo in conversations:\n",
    "#     print(convo)\n",
    "#     print('------------------')\n",
    "\n",
    "# # Tokenize conversations\n",
    "# input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\n",
    "# targets = input_ids.clone()\n",
    "# assert conv.sep_style == conversation_lib.SeparatorStyle.MPT\n",
    "\n",
    "# # Mask targets\n",
    "# sep = conv.sep + conv.roles[1]\n",
    "# for conversation, target in zip(conversations, targets):\n",
    "#     total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "#     rounds = conversation.split(conv.sep)\n",
    "#     re_rounds = [conv.sep.join(rounds[:3])] # system + user + gpt\n",
    "#     for conv_idx in range(3, len(rounds), 2):\n",
    "#         re_rounds.append(conv.sep.join(rounds[conv_idx:conv_idx+2]))    # user + gpt\n",
    "#     cur_len = 0\n",
    "#     target[:cur_len] = IGNORE_INDEX\n",
    "#     for i, rou in enumerate(re_rounds):\n",
    "#         if rou == \"\":\n",
    "#             break\n",
    "\n",
    "#         parts = rou.split(sep)\n",
    "#         if len(parts) != 2:\n",
    "#             break\n",
    "#         parts[0] += sep\n",
    "#         round_len = len(tokenizer_image_token(rou, tokenizer)) + len(tokenizer_image_token(conv.sep, tokenizer))\n",
    "#         instruction_len = len(tokenizer_image_token(parts[0], tokenizer))\n",
    "#         target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "\n",
    "#         cur_len += round_len\n",
    "#     target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "#     if cur_len < tokenizer.model_max_length:\n",
    "#         if cur_len != total_len:\n",
    "#             target[:] = IGNORE_INDEX\n",
    "#             print(\n",
    "#                 f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "#                 f\" (ignored)\"\n",
    "#             )\n",
    "\n",
    "# return dict(\n",
    "#     input_ids=input_ids,\n",
    "#     labels=targets,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.'\n",
    "tok = transformers.AutoTokenizer.from_pretrained('rajammanabrolu/gpt-4-chat', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# convo = [{'role': 'system', 'content': SYSTEM}, \n",
    "#          {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'hi there'}, {'role': 'user', 'content': 'tell me a joke'}, {'role': 'assistant', 'content': 'knock knock...'}]\n",
    "\n",
    "# d1 = {'prompt': tok.apply_chat_template(convo[:2], tokenize=False, add_generation_prompt=True), 'response': convo[2]['content']}\n",
    "# d2 = {'prompt': tok.apply_chat_template(convo[:4], tokenize=False, add_generation_prompt=True), 'response': convo[4]['content']}\n",
    "\n",
    "# format multi-turn data\n",
    "training_data = []\n",
    "for i in range(dataset.num_rows['train']):\n",
    "    # image = os.path.join(image_folder, dataset['train'][i]['image'])\n",
    "    # img = Image.open(image)\n",
    "    # img.show()\n",
    "    convo = dataset[\"train\"][i]['conversations']\n",
    "    # conv = conversation_lib.default_conversation.copy()\n",
    "    # if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "    if convo[0]['from'] != 'human':\n",
    "        convo = convo[1:]\n",
    "        \n",
    "    formatted_convo = [{'role': 'system', 'content': SYSTEM}]\n",
    "    for line in convo:\n",
    "        if line['from'] == 'human':\n",
    "            formatted_convo.append({'role': 'user', 'content': line['value']})\n",
    "        elif line['from'] == 'gpt':\n",
    "            formatted_convo.append({'role': 'assistant', 'content': line['value']})\n",
    "        else:\n",
    "            assert False, 'unrecognized from: %s'%line['from']\n",
    "\n",
    "    # print(formatted_convo)\n",
    "    for j in range(len(formatted_convo)//2):\n",
    "        data_pt = {'prompt': tok.apply_chat_template(formatted_convo[:2*(j+1)], tokenize=False, add_generation_prompt=True), \n",
    "                   'response': formatted_convo[2*(j+1)]['content']}\n",
    "        training_data.append(data_pt)\n",
    "        # print(data_pt)\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(training_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok = transformers.AutoTokenizer.from_pretrained('rajammanabrolu/gpt-4-chat', trust_remote_code=True)\n",
    "\n",
    "# def preprocess_prompt(prompt: str, system=None):\n",
    "#     if system is None:\n",
    "#         system = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.'\n",
    "#     s = tok.apply_chat_template(\n",
    "#         [\n",
    "#             {'role': 'system', 'content': system},\n",
    "#             {'role': 'user', 'content': prompt},\n",
    "#         ],\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True,\n",
    "#     )\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.'\n",
    "\n",
    "tok = transformers.AutoTokenizer.from_pretrained('rajammanabrolu/gpt-4-chat', trust_remote_code=True)\n",
    "\n",
    "convo = [{'role': 'system', 'content': SYSTEM}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'hi there'}, {'role': 'user', 'content': 'tell me a joke'}, {'role': 'assistant', 'content': 'knock knock...'}]\n",
    "\n",
    "\n",
    "# for line in convo:\n",
    "    # print(preprocess_prompt(line['content']))\n",
    "\n",
    "d1 = {'prompt': tok.apply_chat_template(convo[:2], tokenize=False, add_generation_prompt=True), 'response': convo[2]['content']}\n",
    "d2 = {'prompt': tok.apply_chat_template(convo[:4], tokenize=False, add_generation_prompt=True), 'response': convo[4]['content']}\n",
    "\n",
    "print(d1)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    image = os.path.join(image_folder, dataset['train'][i]['image'])\n",
    "    img = Image.open(image)\n",
    "    img.show()\n",
    "    convos = dataset[\"train\"][i]['conversations']\n",
    "    for convo in convos:\n",
    "        print(convo)\n",
    "\n",
    "    print(len(convos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
