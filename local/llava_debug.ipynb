{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import yaml\n",
    "import requests\n",
    "from transformers import AutoProcessor,  AutoTokenizer # LlavaForConditionalGeneration,\n",
    "from omegaconf import OmegaConf as om\n",
    "from tqdm import tqdm \n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import copy\n",
    "import re\n",
    "import torch\n",
    "import streaming\n",
    "from omegaconf import DictConfig, ListConfig\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"/mnt/workdisk/jasmine/llm-foundry\")\n",
    "# from scripts.train.train import validate_config, build_composer_model\n",
    "# from llmfoundry.data.dataloader import build_dataloader\n",
    "# from llmfoundry.utils.config_utils import (log_config, pop_config,\n",
    "#                                            process_init_device,\n",
    "#                                            update_batch_size_info)\n",
    "# from llmfoundry.utils.builders import (add_metrics_to_eval_loaders,\n",
    "#                                        build_algorithm, build_callback,\n",
    "#                                        build_evaluators, build_logger,\n",
    "#                                        build_optimizer, build_scheduler,\n",
    "#                                        build_tokenizer)\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.38.2 in /usr/lib/python3/dist-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers==4.38.2) (3.13.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.38.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/lib/python3/dist-packages (from transformers==4.38.2) (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers==4.38.2) (1.26.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/lib/python3/dist-packages (from transformers==4.38.2) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.38.2) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/lib/python3/dist-packages (from transformers==4.38.2) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/lib/python3/dist-packages (from transformers==4.38.2) (0.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/lib/python3/dist-packages (from transformers==4.38.2) (2023.12.25)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers==4.38.2) (22.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==4.38.2) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.38.2) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.38.2) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3/dist-packages (from requests->transformers==4.38.2) (3.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.38.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "chat_template =  \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif USE_DEFAULT_PROMPT == true and not 'system' in messages[0]['role'] %}{% set loop_messages = messages %}{% set system_message = 'DEFAULT_SYSTEM_PROMPT' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{% if system_message != false %}{{ '<|im_start|>system\\n' + system_message.strip() + '<|im_end|>\\n'}}{% endif %}{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' }}{% else %}{{ '\\n' + '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' }}{% endif %}{% if (add_generation_prompt == true and loop.last) %}{{ '\\n' + '<|im_start|>' + 'assistant' + '\\n' }}{% endif %}{% endfor %}\"\n",
    "tokenizer.chat_template = chat_template\n",
    "tokenizer.add_tokens(['<image>', '<pad>', '<|im_end|>', '<|im_start|>'], special_tokens=True)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.'}, {'role': 'user', 'content': '<image>\\nDo you see any buses or cars?\\nAnswer the question using a single word or phrase.'}, {'role': 'assistant', 'content': 'Yes'}, {'role': 'user', 'content': 'Is there any fence in this photograph?'}, {'role': 'assistant', 'content': 'Yes'}, {'role': 'user', 'content': 'Do you see a bus that is not red?'}, {'role': 'assistant', 'content': 'No'}, {'role': 'user', 'content': 'Which color is that bus?'}, {'role': 'assistant', 'content': 'Red'}, {'role': 'user', 'content': 'What is the sidewalk made of?'}, {'role': 'assistant', 'content': 'Concrete'}]\n"
     ]
    }
   ],
   "source": [
    "example = {'messages': [{'role': 'system', 'content': 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.'}, {'role': 'user', 'content': '<image>\\nDo you see any buses or cars?\\nAnswer the question using a single word or phrase.'}, {'role': 'assistant', 'content': 'Yes'}, {'role': 'user', 'content': 'Is there any fence in this photograph?'}, {'role': 'assistant', 'content': 'Yes'}, {'role': 'user', 'content': 'Do you see a bus that is not red?'}, {'role': 'assistant', 'content': 'No'}, {'role': 'user', 'content': 'Which color is that bus?'}, {'role': 'assistant', 'content': 'Red'}, {'role': 'user', 'content': 'What is the sidewalk made of?'}, {'role': 'assistant', 'content': 'Concrete'}]}\n",
    "messages = example['messages']\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "tokenized_conversation = tokenizer(full_conversation)\n",
    "\n",
    "\n",
    "\n",
    "input_id = torch.tensor(tokenized_conversation['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.<|im_end|>\n",
      "<|im_start|>user\n",
      "<image>\n",
      "Do you see any buses or cars?\n",
      "Answer the question using a single word or phrase.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Yes<|im_end|>\n",
      "<|im_start|>user\n",
      "Is there any fence in this photograph?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Yes<|im_end|>\n",
      "<|im_start|>user\n",
      "Do you see a bus that is not red?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "No<|im_end|>\n",
      "<|im_start|>user\n",
      "Which color is that bus?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Red<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the sidewalk made of?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Concrete<|im_end|>\n",
      "tensor([    1, 32003,  1587,    13,  1976,   460,   264, 10865, 28725,  3116,\n",
      "         1007,   304,  6858, 13892, 28723, 17484,  4372,   390,  1316,  3071,\n",
      "          390,  2572, 28723, 32002, 28705,    13, 32003,  2188,    13, 32000,\n",
      "        28705,    13,  4957,   368,  1032,   707,  1579,   274,   442,  8300,\n",
      "        28804,    13,  2820, 16981,   272,  2996,  1413,   264,  2692,  1707,\n",
      "          442, 14804, 28723, 32002, 28705,    13, 32003, 13892,    13,  5613,\n",
      "        32002, 28705,    13, 32003,  2188,    13,  2301,   736,   707, 17210,\n",
      "          297,   456,  9180, 28804, 32002, 28705,    13, 32003, 13892,    13,\n",
      "         5613, 32002, 28705,    13, 32003,  2188,    13,  4957,   368,  1032,\n",
      "          264,  1579,   369,   349,   459,  2760, 28804, 32002, 28705,    13,\n",
      "        32003, 13892,    13,  2501, 32002, 28705,    13, 32003,  2188,    13,\n",
      "        26703,  3181,   349,   369,  1579, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,  7516, 32002, 28705,    13, 32003,  2188,    13,  3195,\n",
      "          349,   272, 27430,  1269,   302, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,   856,  7916, 32002])\n"
     ]
    }
   ],
   "source": [
    "print(full_conversation)\n",
    "print(input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(input_ids, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets before tensor([    1, 32003,  1587,    13,  1976,   460,   264, 10865, 28725,  3116,\n",
      "         1007,   304,  6858, 13892, 28723, 17484,  4372,   390,  1316,  3071,\n",
      "          390,  2572, 28723, 32002, 28705,    13, 32003,  2188,    13, 32000,\n",
      "        28705,    13,  4957,   368,  1032,   707,  1579,   274,   442,  8300,\n",
      "        28804,    13,  2820, 16981,   272,  2996,  1413,   264,  2692,  1707,\n",
      "          442, 14804, 28723, 32002, 28705,    13, 32003, 13892,    13,  5613,\n",
      "        32002, 28705,    13, 32003,  2188,    13,  2301,   736,   707, 17210,\n",
      "          297,   456,  9180, 28804, 32002, 28705,    13, 32003, 13892,    13,\n",
      "         5613, 32002, 28705,    13, 32003,  2188,    13,  4957,   368,  1032,\n",
      "          264,  1579,   369,   349,   459,  2760, 28804, 32002, 28705,    13,\n",
      "        32003, 13892,    13,  2501, 32002, 28705,    13, 32003,  2188,    13,\n",
      "        26703,  3181,   349,   369,  1579, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,  7516, 32002, 28705,    13, 32003,  2188,    13,  3195,\n",
      "          349,   272, 27430,  1269,   302, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,   856,  7916, 32002])\n",
      "cur len 23\n",
      "--\n",
      "cur len 51\n",
      "--\n",
      "cur len 56\n",
      "--\n",
      "cur len 68\n",
      "--\n",
      "cur len 73\n",
      "--\n",
      "cur len 87\n",
      "--\n",
      "cur len 92\n",
      "--\n",
      "cur len 102\n",
      "--\n",
      "cur len 107\n",
      "--\n",
      "cur len 118\n",
      "--\n",
      "cur len 125\n",
      "--\n",
      "targets after tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100, 14804, 28723, 32002, 28705,    13,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   707, 17210,\n",
      "          297,   456,  9180,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  4957,   368,  1032,\n",
      "          264,  1579,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,    13,  2501, 32002, 28705,    13,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,    13, 32003,\n",
      "        13892,    13,  7516, 32002, 28705,    13, 32003,  2188,    13,  3195,\n",
      "          349,   272, 27430,  1269,   302, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,   856,  7916, 32002])\n"
     ]
    }
   ],
   "source": [
    "# trying to split based off of pre-tokenized output\n",
    "\n",
    "IMAGE_TOKEN_INDEX = 32000 # i think\n",
    "_HF_IGNORE_INDEX = -100\n",
    "sep = '<|im_end|>\\n' \n",
    "start = '<|im_start|>'\n",
    "sep_n_tokens = 2 # NOTE: check if '<|im_end|>\\n' splits into two tokens bc will need that for later\n",
    "\n",
    "rounds = full_conversation.split(sep) \n",
    "cur_len = 0 #  1 ??\n",
    "# target[:cur_len] = _HF_IGNORE_INDEX\n",
    "\n",
    "target = input_id.clone() # copy.deepcopy(input_id)\n",
    "print('targets before', target)\n",
    "for i, rou in enumerate(rounds):\n",
    "    role = rou.split('\\n')[0][len(start):] # split on first newline then grab everything after <|im_start|>\n",
    "    # print(role)\n",
    "    # print(rou)\n",
    "\n",
    "    round_len = len(tokenizer_image_token(rou, tokenizer))\n",
    "    if role == 'assistant':\n",
    "        # need to separate <|im_start|>assistant\\n from the rest of stuff\n",
    "        pass\n",
    "        # round_len\n",
    "        # instruction_len = 0\n",
    "    else: # system or user\n",
    "        \n",
    "        target[cur_len: cur_len + round_len] = _HF_IGNORE_INDEX\n",
    "        # print(round_len)\n",
    "\n",
    "    cur_len += round_len\n",
    "    print('cur len', cur_len)\n",
    "\n",
    "    print('--')\n",
    "    # break\n",
    "print('targets after', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32003\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids('<|im_start|>'))\n",
    "print(tokenizer.convert_tokens_to_ids('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t'<s>'\n",
      "32003\t'<|im_start|>'\n",
      "1587\t'system'\n",
      "13\t'\\n'\n",
      "1976\t'You'\n",
      "460\t'are'\n",
      "264\t'a'\n",
      "10865\t'helpful'\n",
      "28725\t','\n",
      "3116\t'respect'\n",
      "1007\t'ful'\n",
      "304\t'and'\n",
      "6858\t'honest'\n",
      "13892\t'assistant'\n",
      "28723\t'.'\n",
      "17484\t'Always'\n",
      "4372\t'answer'\n",
      "390\t'as'\n",
      "1316\t'help'\n",
      "3071\t'fully'\n",
      "390\t'as'\n",
      "2572\t'possible'\n",
      "28723\t'.'\n",
      "32002\t'<|im_end|>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "32003\t'<|im_start|>'\n",
      "2188\t'user'\n",
      "13\t'\\n'\n",
      "32000\t'<image>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "4957\t'Do'\n",
      "368\t'you'\n",
      "1032\t'see'\n",
      "707\t'any'\n",
      "1579\t'bus'\n",
      "274\t'es'\n",
      "442\t'or'\n",
      "8300\t'cars'\n",
      "28804\t'?'\n",
      "13\t'\\n'\n",
      "2820\t'An'\n",
      "16981\t'swer'\n",
      "272\t'the'\n",
      "2996\t'question'\n",
      "1413\t'using'\n",
      "264\t'a'\n",
      "2692\t'single'\n",
      "1707\t'word'\n",
      "442\t'or'\n",
      "14804\t'phrase'\n",
      "28723\t'.'\n",
      "32002\t'<|im_end|>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "32003\t'<|im_start|>'\n",
      "13892\t'assistant'\n",
      "13\t'\\n'\n",
      "5613\t'Yes'\n",
      "32002\t'<|im_end|>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "32003\t'<|im_start|>'\n",
      "2188\t'user'\n",
      "13\t'\\n'\n",
      "2301\t'Is'\n",
      "736\t'there'\n",
      "707\t'any'\n",
      "17210\t'fence'\n",
      "297\t'in'\n",
      "456\t'this'\n",
      "9180\t'photograph'\n",
      "28804\t'?'\n",
      "32002\t'<|im_end|>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "32003\t'<|im_start|>'\n",
      "13892\t'assistant'\n",
      "13\t'\\n'\n",
      "5613\t'Yes'\n",
      "32002\t'<|im_end|>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "32003\t'<|im_start|>'\n",
      "2188\t'user'\n",
      "13\t'\\n'\n",
      "4957\t'Do'\n",
      "368\t'you'\n",
      "1032\t'see'\n",
      "264\t'a'\n",
      "1579\t'bus'\n",
      "369\t'that'\n",
      "349\t'is'\n",
      "459\t'not'\n",
      "2760\t'red'\n",
      "28804\t'?'\n",
      "32002\t'<|im_end|>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "32003\t'<|im_start|>'\n",
      "13892\t'assistant'\n",
      "13\t'\\n'\n",
      "2501\t'No'\n",
      "32002\t'<|im_end|>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "32003\t'<|im_start|>'\n",
      "2188\t'user'\n",
      "13\t'\\n'\n",
      "26703\t'Which'\n",
      "3181\t'color'\n",
      "349\t'is'\n",
      "369\t'that'\n",
      "1579\t'bus'\n",
      "28804\t'?'\n",
      "32002\t'<|im_end|>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "32003\t'<|im_start|>'\n",
      "13892\t'assistant'\n",
      "13\t'\\n'\n",
      "7516\t'Red'\n",
      "32002\t'<|im_end|>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "32003\t'<|im_start|>'\n",
      "2188\t'user'\n",
      "13\t'\\n'\n",
      "3195\t'What'\n",
      "349\t'is'\n",
      "272\t'the'\n",
      "27430\t'sidewalk'\n",
      "1269\t'made'\n",
      "302\t'of'\n",
      "28804\t'?'\n",
      "32002\t'<|im_end|>'\n",
      "28705\t''\n",
      "13\t'\\n'\n",
      "32003\t'<|im_start|>'\n",
      "13892\t'assistant'\n",
      "13\t'\\n'\n",
      "856\t'Con'\n",
      "7916\t'crete'\n",
      "32002\t'<|im_end|>'\n"
     ]
    }
   ],
   "source": [
    "# print(input_id.shape)\n",
    "# print(len(tokenizer.batch_decode(input_id)))\n",
    "\n",
    "for tok, decoding in zip(list(input_id), tokenizer.batch_decode(input_id)):\n",
    "    print(str(tok.item()) + '\\t' +  repr(decoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32002\n"
     ]
    }
   ],
   "source": [
    "sep_token = tokenizer.convert_tokens_to_ids('<|im_end|>')\n",
    "print(sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minput_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(sep_token)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets before tensor([    1, 32003,  1587,    13,  1976,   460,   264, 10865, 28725,  3116,\n",
      "         1007,   304,  6858, 13892, 28723, 17484,  4372,   390,  1316,  3071,\n",
      "          390,  2572, 28723, 32002, 28705,    13, 32003,  2188,    13, 32000,\n",
      "        28705,    13,  4957,   368,  1032,   707,  1579,   274,   442,  8300,\n",
      "        28804,    13,  2820, 16981,   272,  2996,  1413,   264,  2692,  1707,\n",
      "          442, 14804, 28723, 32002, 28705,    13, 32003, 13892,    13,  5613,\n",
      "        32002, 28705,    13, 32003,  2188,    13,  2301,   736,   707, 17210,\n",
      "          297,   456,  9180, 28804, 32002, 28705,    13, 32003, 13892,    13,\n",
      "         5613, 32002, 28705,    13, 32003,  2188,    13,  4957,   368,  1032,\n",
      "          264,  1579,   369,   349,   459,  2760, 28804, 32002, 28705,    13,\n",
      "        32003, 13892,    13,  2501, 32002, 28705,    13, 32003,  2188,    13,\n",
      "        26703,  3181,   349,   369,  1579, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,  7516, 32002, 28705,    13, 32003,  2188,    13,  3195,\n",
      "          349,   272, 27430,  1269,   302, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,   856,  7916, 32002])\n",
      "tensor([    1, 32003,  1587,    13,  1976,   460,   264, 10865, 28725,  3116,\n",
      "         1007,   304,  6858, 13892, 28723, 17484,  4372,   390,  1316,  3071,\n",
      "          390,  2572, 28723, 32002, 28705,    13, 32003,  2188,    13, 32000,\n",
      "        28705,    13,  4957,   368,  1032,   707,  1579,   274,   442,  8300,\n",
      "        28804,    13,  2820, 16981,   272,  2996,  1413,   264,  2692,  1707,\n",
      "          442, 14804, 28723, 32002, 28705,    13, 32003, 13892,    13,  5613,\n",
      "        32002, 28705,    13, 32003,  2188,    13,  2301,   736,   707, 17210,\n",
      "          297,   456,  9180, 28804, 32002, 28705,    13, 32003, 13892,    13,\n",
      "         5613, 32002, 28705,    13, 32003,  2188,    13,  4957,   368,  1032,\n",
      "          264,  1579,   369,   349,   459,  2760, 28804, 32002, 28705,    13,\n",
      "        32003, 13892,    13,  2501, 32002, 28705,    13, 32003,  2188,    13,\n",
      "        26703,  3181,   349,   369,  1579, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,  7516, 32002, 28705,    13, 32003,  2188,    13,  3195,\n",
      "          349,   272, 27430,  1269,   302, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,   856,  7916, 32002])\n",
      "--\n",
      "targets after tensor([    1, 32003,  1587,    13,  1976,   460,   264, 10865, 28725,  3116,\n",
      "         1007,   304,  6858, 13892, 28723, 17484,  4372,   390,  1316,  3071,\n",
      "          390,  2572, 28723, 32002, 28705,    13, 32003,  2188,    13, 32000,\n",
      "        28705,    13,  4957,   368,  1032,   707,  1579,   274,   442,  8300,\n",
      "        28804,    13,  2820, 16981,   272,  2996,  1413,   264,  2692,  1707,\n",
      "          442, 14804, 28723, 32002, 28705,    13, 32003, 13892,    13,  5613,\n",
      "        32002, 28705,    13, 32003,  2188,    13,  2301,   736,   707, 17210,\n",
      "          297,   456,  9180, 28804, 32002, 28705,    13, 32003, 13892,    13,\n",
      "         5613, 32002, 28705,    13, 32003,  2188,    13,  4957,   368,  1032,\n",
      "          264,  1579,   369,   349,   459,  2760, 28804, 32002, 28705,    13,\n",
      "        32003, 13892,    13,  2501, 32002, 28705,    13, 32003,  2188,    13,\n",
      "        26703,  3181,   349,   369,  1579, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,  7516, 32002, 28705,    13, 32003,  2188,    13,  3195,\n",
      "          349,   272, 27430,  1269,   302, 28804, 32002, 28705,    13, 32003,\n",
      "        13892,    13,   856,  7916, 32002])\n"
     ]
    }
   ],
   "source": [
    "# trying to split based off of TOKENIZED output\n",
    "\n",
    "# TODO maybe try to build targets (list) based off of appending and switching mode when we see a certain thing :|\n",
    "# but wanna do chunks probably\n",
    "\n",
    "IMAGE_TOKEN_INDEX = tokenizer.convert_tokens_to_ids('<image>') # i think\n",
    "_HF_IGNORE_INDEX = -100\n",
    "sep_token = tokenizer.convert_tokens_to_ids('<|im_end|>')\n",
    "# start = '<|im_start|>'\n",
    "# sep_n_tokens = 2 # NOTE: check if '<|im_end|>\\n' splits into two tokens bc will need that for later\n",
    "\n",
    "rounds = input_id.split(sep_token) \n",
    "cur_len = 0 #  1 ??\n",
    "# target[:cur_len] = _HF_IGNORE_INDEX\n",
    "\n",
    "target = input_id.clone() # copy.deepcopy(input_id)\n",
    "print('targets before', target)\n",
    "for i, rou in enumerate(rounds):\n",
    "    print(rou)\n",
    "\n",
    "    # cur_len += round_len\n",
    "    # print('cur len', cur_len)\n",
    "\n",
    "    print('--')\n",
    "    # break\n",
    "print('targets after', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add end signal and concatenate together\n",
    "conversations = []\n",
    "for source in sources:\n",
    "    assert len(source) == 2\n",
    "    assert DEFAULT_IMAGE_TOKEN in source[0]['value']\n",
    "    source[0]['value'] = DEFAULT_IMAGE_TOKEN\n",
    "    conversation = source[0]['value'] + source[1]['value'] + conversation_lib.default_conversation.sep\n",
    "    conversations.append(conversation)\n",
    "# tokenize conversations\n",
    "input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n",
    "targets = copy.deepcopy(input_ids)\n",
    "for target, source in zip(targets, sources):\n",
    "    tokenized_len = len(tokenizer_image_token(source[0]['value'], tokenizer))\n",
    "    target[:tokenized_len] = IGNORE_INDEX\n",
    "\n",
    "return dict(input_ids=input_ids, labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1 = AutoTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\n",
    "vicuna_chat_template = \"{% if messages[0]['role'] == 'system' %} {% set loop_messages = messages[1:] %} {% set system_message = messages[0]['content'].strip() + '\\n\\n' %} {% else %} {% set loop_messages = messages %} {% set system_message = '' %} {% endif %} {{ bos_token + system_message }} {% for message in loop_messages %} {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %} {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }} {% endif %} {% if message['role'] == 'user' %} {{ 'USER: ' + message['content'].strip() + '\\n' }} {% elif message['role'] == 'assistant' %} {{ 'ASSISTANT: ' + message['content'].strip() + eos_token + '\\n' }} {% endif %} {% endfor %} {% if add_generation_prompt %} {{ 'ASSISTANT:' }} {% endif %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "if tokenizer_1.chat_template:\n",
    "    print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1.chat_template = vicuna_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    <s>    USER: Hello, how are you?\n",
      "     ASSISTANT: I'm doing great. How can I help you today?</s>\n",
      "     USER: I'd like to show off how chat templating works!\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "   {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "out = tokenizer_1.apply_chat_template(chat, tokenize=False)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://us.feliway.com/cdn/shop/articles/10_fascinating_facts_about_black_cats-3.jpg?v=1667409596&width=19201x//us.feliway.com/cdn/shop/articles/10_fascinating_facts_about_black_cats-3.jpg?v=1667409596&width=38402x\"\n",
    "# url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessor_config.json: 100%|██████████| 505/505 [00:00<00:00, 3.21MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.41k/1.41k [00:00<00:00, 11.1MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 20.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 20.9MB/s]\n",
      "added_tokens.json: 100%|██████████| 41.0/41.0 [00:00<00:00, 362kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 4.95MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"llava-hf/bakLlava-v1-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  1, 843, 912, 843, 912]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'pixel_values': None}\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(text='blah blah', images=None, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_ids = torch.tensor([[    1,   523, 28766,   321, 28730,  2521, 28766, 28767,  1587,    13,                                                                                           \n",
    "          1976,   460,   264, 10865, 28725,  3116,  1007,   304,  6858, 13892,                                                                                                     \n",
    "         28723, 17484,  4372,   390,  1316,  3071,   390,  2572, 26364, 28766,                                                                                                     \n",
    "           321, 28730,   416, 28766, 28767, 28705,    13, 28789, 28766,   321,                                                                                                     \n",
    "         28730,  2521, 28766, 28767,  2188,    13, 32000,   259,    13,  3195,                                                                                                     \n",
    "         28742, 28713,   272,  3036,   302,   272,  3469, 28804, 28789, 28766,                                                                                                     \n",
    "           321, 28730,   416, 28766, 28767, 28705,    13, 28789, 28766,   321,                                                                                                     \n",
    "         28730,  2521, 28766, 28767, 13892,    13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='llava-hf/bakLlava-v1-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<s> <|im_start|> system\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible.<|im_end|> \\n<|im_start|> user\\n<image>  \\nWhat's the content of the image?<|im_end|> \\n<|im_start|> assistant\\n\"\n",
      "\"<s>-<|im_start|>-system\\nYou-are-a-helpful,-respectful-and-honest-assistant.-Always-answer-as-helpfully-as-possible.<|im_end|>-\\n<|im_start|>-user\\n<image>--\\nWhat's-the-content-of-the-image?<|im_end|>-\\n<|im_start|>-assistant\\n\"\n"
     ]
    }
   ],
   "source": [
    "res = processor.batch_decode(model_input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\n",
    "print(repr(res))\n",
    "print(repr(res.replace(' ', '-')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<s>-<|im_start|>-system\\nYou-are-a-helpful,-respectful-and-honest-assistant.-Always-answer-as-helpfully-as-possible.<|im_end|>-\\n<|im_start|>-user\\n<image>--\\nWhat's-the-content-of-the-image?<|im_end|>-\\n<|im_start|>-assistant\\n\"\n",
      "'<s>-<|im_start|>-system\\nYou-are-a-helpful,-respectful-and-honest-assistant.-Always-answer-as-helpfully-as-possible.<|im_end|>-\\n<|im_start|>-user\\n<image>----\\nWhat-credit-card-company-is-on-the-banner-in-the-background?\\nAnswer-the-question-using-a-single-word-or-phrase.<|im_end|>-\\n<|im_start|>-assistant\\n'\n"
     ]
    }
   ],
   "source": [
    "print(repr(res.replace(' ', '-')))\n",
    "other_model_input_ids = torch.tensor([[    1,   523, 28766,   321, 28730,  2521, 28766, 28767,  1587,    13,                                                                                           \n",
    "          1976,   460,   264, 10865, 28725,  3116,  1007,   304,  6858, 13892,                                                                                                     \n",
    "         28723, 17484,  4372,   390,  1316,  3071,   390,  2572, 26364, 28766,                                                                                                     \n",
    "           321, 28730,   416, 28766, 28767, 28705,    13, 28789, 28766,   321,                                                                                                     \n",
    "         28730,  2521, 28766, 28767,  2188,    13, 32000,   260,    13,  3195,                                                                                                     \n",
    "          6183,  4148,  2496,   349,   356,   272,   287,  7041,   297,   272,                                                                                                     \n",
    "          5414, 28804,    13,  2820, 16981,   272,  2996,  1413,   264,  2692,                                                                                                     \n",
    "          1707,   442, 14804, 26364, 28766,   321, 28730,   416, 28766, 28767,                                                                                                     \n",
    "         28705,    13, 28789, 28766,   321, 28730,  2521, 28766, 28767, 13892,                                                                                                     \n",
    "            13]])\n",
    "\n",
    "res = processor.batch_decode(other_model_input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\n",
    "print(repr(res.replace(' ', '-')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoConfig, PreTrainedTokenizerBase, AutoModel,\n",
    "                          LlavaForConditionalGeneration, LlavaConfig,\n",
    "                          CLIPVisionConfig, MistralConfig, LlamaConfig, AutoTokenizer)\n",
    "\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained('llava-hf/bakLlava-v1-hf')\n",
    "\n",
    "tokenizer_1_add_tokens = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "tokenizer_1_add_tokens.add_tokens(['<image>', '<pad>'], special_tokens=True)\n",
    "\n",
    "tokenizer_1.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_1)\n",
    "\n",
    "tokenizer_1.add_tokens(['<image>', '<pad>'], special_tokens=True)\n",
    "\n",
    "print(tokenizer_1)\n",
    "\n",
    "tokenizer_1.add_tokens(['<image>', '<pad>'], special_tokens=True)\n",
    "\n",
    "print(tokenizer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/lib/python3/dist-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/lib/python3/dist-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2.8)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "added_tokens.json: 100%|██████████| 74.0/74.0 [00:00<00:00, 612kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 200/200 [00:00<00:00, 1.39MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TiktokenTokenizerWrapper(name_or_path='rajammanabrolu/gpt-4-chat', vocab_size=100277, model_max_length=8192, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t100257: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100277: AddedToken(\"<|pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100278: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100279: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! pip install tiktoken\n",
    "tok = AutoTokenizer.from_pretrained('rajammanabrolu/gpt-4-chat', trust_remote_code=True)\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_1.add_special_tokens({'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>'}) # do we want to do this???\n",
    "\n",
    "tokenizer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original conversation:\n",
      "{'from': 'human', 'value': '<image>\\nWhat are the colors of the bus in the image?'}\n",
      "{'from': 'gpt', 'value': 'The bus in the image is white and red.'}\n",
      "{'from': 'human', 'value': 'What feature can be seen on the back of the bus?'}\n",
      "{'from': 'gpt', 'value': 'The back of the bus features an advertisement.'}\n",
      "{'from': 'human', 'value': 'Is the bus driving down the street or pulled off to the side?'}\n",
      "{'from': 'gpt', 'value': 'The bus is driving down the street, which is crowded with people and other vehicles.'}\n",
      "3 formatted convos\n"
     ]
    }
   ],
   "source": [
    "SYSTEM = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.'\n",
    "\n",
    "formatted_chat_dicts = []\n",
    "for i in range(len(sample['conversations']) // 2): # make a formatted convo for each turn\n",
    "    # print((i+1)*2)\n",
    "    # print(sample['conversations'][:(i+1)*2])\n",
    "    # print()\n",
    "    formatted_convo = [{'role': 'system', 'content': SYSTEM}]\n",
    "    for line in sample['conversations'][:(i+1)*2]:\n",
    "        if line['from'] == 'human':\n",
    "            formatted_convo.append({'role': 'user', 'content': line['value']})\n",
    "        elif line['from'] == 'gpt':\n",
    "            formatted_convo.append({'role': 'assistant', 'content': line['value']})\n",
    "    formatted_chat_dicts.append({'messages': formatted_convo})\n",
    "\n",
    "\n",
    "print('original conversation:')\n",
    "for line in sample['conversations']:\n",
    "    print(line)\n",
    "\n",
    "# chat_formatted_dict = {'messages': formatted_convos}\n",
    "# print(chat_formatted_dict)\n",
    "\n",
    "print(len(formatted_chat_dicts), 'formatted convos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: <|im_start|>system\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.<|im_end|>\n",
      "<|im_start|>user\n",
      "<image>\n",
      "What are the colors of the bus in the image?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The bus in the image is white and red.<|im_end|>\n",
      "<|im_start|>user\n",
      "What feature can be seen on the back of the bus?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The back of the bus features an advertisement.<|im_end|>\n",
      "<|im_start|>user\n",
      "Is the bus driving down the street or pulled off to the side?<|im_end|>\n",
      "\n",
      "RESPONSE: <|im_start|>assistant\n",
      "The bus is driving down the street, which is crowded with people and other vehicles.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmfoundry.data.finetuning.tasks import _slice_chat_formatted_example\n",
    "prompt, response = _slice_chat_formatted_example(formatted_chat_dicts[2], tokenizer_1)\n",
    "\n",
    "print('PROMPT:', prompt)\n",
    "print('RESPONSE:', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT <|im_start|>system\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.<|im_end|>\n",
      "<|im_start|>user\n",
      "<image>\n",
      "What are the colors of the bus in the image?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The bus in the image is white and red.<|im_end|>\n",
      "<|im_start|>user\n",
      "What feature can be seen on the back of the bus?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The back of the bus features an advertisement.<|im_end|>\n",
      "<|im_start|>user\n",
      "Is the bus driving down the street or pulled off to the side?<|im_end|>\n",
      "\n",
      "RESPONSE: <|im_start|>assistant\n",
      "The bus is driving down the street, which is crowded with people and other vehicles.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='llava-hf/bakLlava-v1-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer_1_add_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(577, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_tower_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionConfig {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"hidden_act\": \"quick_gelu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 336,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"model_type\": \"clip_vision_model\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 14,\n",
      "  \"projection_dim\": 768,\n",
      "  \"transformers_version\": \"4.37.2\"\n",
      "}\n",
      "\n",
      "MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-v0.1\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, CLIPVisionConfig\n",
    "\n",
    "vision_tower = 'openai/clip-vit-large-patch14-336'\n",
    "vision_config = CLIPVisionConfig.from_pretrained(vision_tower)\n",
    "\n",
    "print(vision_config)\n",
    "\n",
    "\n",
    "\n",
    "model_name_or_path = 'mistralai/Mistral-7B-v0.1'\n",
    "text_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "print(text_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPConfig {\n",
       "  \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
       "  \"architectures\": [\n",
       "    \"CLIPModel\"\n",
       "  ],\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"logit_scale_init_value\": 2.6592,\n",
       "  \"model_type\": \"clip\",\n",
       "  \"projection_dim\": 768,\n",
       "  \"text_config\": {\n",
       "    \"dropout\": 0.0,\n",
       "    \"hidden_size\": 768,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"model_type\": \"clip_text_model\",\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"projection_dim\": 768\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"vision_config\": {\n",
       "    \"dropout\": 0.0,\n",
       "    \"hidden_size\": 1024,\n",
       "    \"image_size\": 336,\n",
       "    \"intermediate_size\": 4096,\n",
       "    \"model_type\": \"clip_vision_model\",\n",
       "    \"num_attention_heads\": 16,\n",
       "    \"num_hidden_layers\": 24,\n",
       "    \"patch_size\": 14,\n",
       "    \"projection_dim\": 768\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaConfig {\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_token_index\": 32000,\n",
      "  \"model_type\": \"llava\",\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"mistralai/Mistral-7B-v0.1\",\n",
      "    \"architectures\": [\n",
      "      \"MistralForCausalLM\"\n",
      "    ],\n",
      "    \"intermediate_size\": 14336,\n",
      "    \"max_position_embeddings\": 32768,\n",
      "    \"model_type\": \"mistral\",\n",
      "    \"num_key_value_heads\": 8,\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"sliding_window\": 4096,\n",
      "    \"torch_dtype\": \"bfloat16\"\n",
      "  },\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"vision_config\": {\n",
      "    \"dropout\": 0.0,\n",
      "    \"hidden_size\": 1024,\n",
      "    \"image_size\": 336,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"patch_size\": 14,\n",
      "    \"projection_dim\": 768\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\",\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llava_config = LlavaConfig(vision_config, text_config)\n",
    "print(llava_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaConfig {\n",
       "  \"ignore_index\": -100,\n",
       "  \"image_token_index\": 32000,\n",
       "  \"model_type\": \"llava\",\n",
       "  \"projector_hidden_act\": \"gelu\",\n",
       "  \"text_config\": {\n",
       "    \"_name_or_path\": \"mistralai/Mistral-7B-v0.1\",\n",
       "    \"architectures\": [\n",
       "      \"MistralForCausalLM\"\n",
       "    ],\n",
       "    \"intermediate_size\": 14336,\n",
       "    \"max_position_embeddings\": 32768,\n",
       "    \"model_type\": \"mistral\",\n",
       "    \"num_key_value_heads\": 8,\n",
       "    \"rms_norm_eps\": 1e-05,\n",
       "    \"sliding_window\": 4096,\n",
       "    \"torch_dtype\": \"bfloat16\"\n",
       "  },\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"vision_config\": {\n",
       "    \"dropout\": 0.0,\n",
       "    \"hidden_size\": 1024,\n",
       "    \"image_size\": 336,\n",
       "    \"intermediate_size\": 4096,\n",
       "    \"model_type\": \"clip_vision_model\",\n",
       "    \"num_attention_heads\": 16,\n",
       "    \"num_hidden_layers\": 24,\n",
       "    \"patch_size\": 14,\n",
       "    \"projection_dim\": 768\n",
       "  },\n",
       "  \"vision_feature_layer\": -2,\n",
       "  \"vision_feature_select_strategy\": \"default\",\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava_config # .text_config._name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_seq_len': 1024, 'global_seed': 17, 'model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'run_name': 'bakllava-cc3m-clip-llm-pretrained-refactor', 'model': {'name': 'hf_multimodal_causal_lm', 'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'llm_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'vision_model_name_or_path': 'openai/clip-vit-large-patch14-336', 'pretrained': False, 'train_llm': False}, 'tokenizer': {'name': 'mistralai/Mistral-7B-v0.1', 'kwargs': {'model_max_length': 1024, 'chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif USE_DEFAULT_PROMPT == true and not 'system' in messages[0]['role'] %}{% set loop_messages = messages %}{% set system_message = 'DEFAULT_SYSTEM_PROMPT' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{% if system_message != false %}{{ '<|im_start|>system\\n' + system_message.strip() + '<|im_end|>\\n'}}{% endif %}{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' }}{% else %}{{ '\\n' + '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' }}{% endif %}{% if (add_generation_prompt == true and loop.last) %}{{ '\\n' + '<|im_start|>' + 'assistant' + '\\n' }}{% endif %}{% endfor %}\", 'additional_special_tokens': ['<image>', '<pad>', '<|im_end|>', '<|im_start|>']}}, 'train_loader': {'name': 'finetuning', 'dataset': {'remote': 'oci://mosaicml-internal-dataset-llava/LLaVA-Mix-FT-665K-Chat', 'local': '/tmp/mds-cache-train/', 'shuffle': True, 'max_seq_len': 1024, 'decoder_only_format': True, 'num_canonical_nodes': 1}, 'drop_last': True, 'num_workers': 8}, 'scheduler': {'name': 'cosine_with_warmup', 't_warmup': '100ba', 'alpha_f': 0.1}, 'optimizer': {'name': 'decoupled_adamw', 'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-08, 'weight_decay': 0.0}, 'algorithms': {'gradient_clipping': {'clipping_type': 'norm', 'clipping_threshold': 1.0}}, 'max_duration': '1ep', 'eval_interval': 1, 'eval_first': False, 'eval_subset_num_batches': -1, 'global_train_batch_size': 4, 'seed': 17, 'device_eval_batch_size': 1, 'device_train_microbatch_size': 1, 'precision': 'amp_bf16', 'fsdp_config': {'sharding_strategy': 'FULL_SHARD', 'mixed_precision': 'PURE', 'activation_checkpointing': True, 'activation_checkpointing_reentrant': False, 'activation_cpu_offload': False, 'limit_all_gathers': True, 'use_orig_params': True}, 'progress_bar': False, 'log_to_console': True, 'console_log_interval': '1ba', 'callbacks': {'speed_monitor': {'window_size': 10}, 'lr_monitor': {}, 'memory_monitor': {}, 'runtime_estimator': {}, 'optimizer_monitor': {}}, 'loggers': {'wandb': {'name': 'bakllava-cc3m-clip-llm-pretrained-refactor', 'project': 'jasmine-llava', 'group': 'bakllava-cc3m-clip-llm-pretrained-refactor'}}}\n"
     ]
    }
   ],
   "source": [
    "yaml_path = '/mnt/workdisk/jasmine/yamls/llava/mpt-7b-llava-local.yaml'\n",
    "cfg = om.load(yaml_path)\n",
    "\n",
    "validate_config(cfg)\n",
    "om.resolve(cfg)\n",
    "print(cfg)\n",
    "\n",
    "# Mandatory model training configs\n",
    "model_config: DictConfig = pop_config(cfg, 'model', must_exist=True)\n",
    "tokenizer_config: Dict[str, Any] = pop_config(cfg,\n",
    "                                                'tokenizer',\n",
    "                                                must_exist=True,\n",
    "                                                convert=True)\n",
    "optimizer_config: Dict[str, Any] = pop_config(cfg,\n",
    "                                                'optimizer',\n",
    "                                                must_exist=True,\n",
    "                                                convert=True)\n",
    "scheduler_config: Dict[str, Any] = pop_config(cfg,\n",
    "                                                'scheduler',\n",
    "                                                must_exist=True,\n",
    "                                                convert=True)\n",
    "train_loader_config: DictConfig = pop_config(cfg,\n",
    "                                                'train_loader',\n",
    "                                                must_exist=True)\n",
    "max_duration: Union[int, str] = pop_config(cfg,\n",
    "                                            'max_duration',\n",
    "                                            must_exist=True)\n",
    "eval_interval: Union[int, str] = pop_config(cfg,\n",
    "                                            'eval_interval',\n",
    "                                            must_exist=True)\n",
    "precision: str = pop_config(cfg, 'precision', must_exist=True)\n",
    "max_seq_len: int = pop_config(cfg, 'max_seq_len', must_exist=True)\n",
    "device_train_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'hf_multimodal_causal_lm', 'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'llm_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'vision_model_name_or_path': 'openai/clip-vit-large-patch14-336', 'pretrained': False, 'train_llm': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).\n"
     ]
    }
   ],
   "source": [
    "# Build tokenizer\n",
    "tokenizer_name = tokenizer_config['name']\n",
    "tokenizer_kwargs = tokenizer_config.get('kwargs', {})\n",
    "tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)\n",
    "\n",
    "# Build train loader\n",
    "streaming.base.util.clean_stale_shared_memory()\n",
    "train_loader = build_dataloader(\n",
    "    train_loader_config,\n",
    "    tokenizer,\n",
    "    device_train_batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/transformers/models/auto/configuration_auto.py:1085: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n params 7566743552\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = build_composer_model(model_config, tokenizer)\n",
    "\n",
    "if model_config.get('master_weights_dtype') in ('bf16', 'bfloat16'):\n",
    "    model = model.to(dtype=torch.bfloat16)\n",
    "elif model_config.get('master_weights_dtype') in ('f16', 'float16'):\n",
    "    model = model.to(dtype=torch.float16)\n",
    "\n",
    "# Log number of parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print('n params', n_params)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_name: str = optimizer_config.pop('name')\n",
    "optimizer = build_optimizer(model, optimizer_name, optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.vision_tower.requires_grad_(False)\n",
    "# model.model.language_model.requires_grad_(False)\n",
    "# model.model.multi_modal_projector.requires_grad_(True) # explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, p in model.named_parameters():\n",
    "#     print(name, 'requires grad', p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 512])\n",
      "attention_mask torch.Size([1, 512])\n",
      "labels torch.Size([1, 512])\n",
      "llava_labels torch.Size([1, 86])\n",
      "bidirectional_mask torch.Size([1, 512])\n",
      "pixel_values torch.Size([1, 3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.load('batch.ckpt')\n",
    "\n",
    "for key in batch:\n",
    "    print(key, batch[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.metrics.nlp import LanguageCrossEntropy\n",
    "from torch import nn\n",
    "_HF_IGNORE_INDEX = -100\n",
    "\n",
    "composer_xent = LanguageCrossEntropy(ignore_index=_HF_IGNORE_INDEX)\n",
    "\n",
    "torch_xent = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 39.39 GiB of which 109.94 MiB is free. Process 1180216 has 39.27 GiB memory in use. Of the allocated memory 38.06 GiB is allocated by PyTorch, and 720.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m out, aug_label \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/workdisk/jasmine/llm-foundry/llmfoundry/models/hf/hf_llava.py:169\u001b[0m, in \u001b[0;36mComposerHFLLaVa.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, UserDict):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Further input validation is left to the huggingface forward call\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    167\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward_args\n\u001b[1;32m    168\u001b[0m     }\n\u001b[0;32m--> 169\u001b[0m     output, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore (thirdparty)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# TODO not the right fix, but trying\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     output\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits[:,\u001b[38;5;241m-\u001b[39mmax_seq_len:]\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/transformers/models/llava/modeling_llava.py:469\u001b[0m, in \u001b[0;36mLlavaForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    466\u001b[0m             attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((attention_mask, extended_attention_mask), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    467\u001b[0m             position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(attention_mask, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 469\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    482\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/transformers/models/mistral/modeling_mistral.py:1154\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1151\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1154\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1167\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/transformers/models/mistral/modeling_mistral.py:1039\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1030\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1031\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         use_cache,\n\u001b[1;32m   1037\u001b[0m     )\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1039\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/transformers/models/mistral/modeling_mistral.py:754\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    764\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/transformers/models/mistral/modeling_mistral.py:300\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    301\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    302\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 39.39 GiB of which 109.94 MiB is free. Process 1180216 has 39.27 GiB memory in use. Of the allocated memory 38.06 GiB is allocated by PyTorch, and 720.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = model.half()\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "for i, key in enumerate(batch):\n",
    "    batch[key] = batch[key].to('cuda')\n",
    "    if i==2:\n",
    "        break\n",
    "\n",
    "out, aug_label = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5281, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(17.7352)\n",
      "tensor(0.5281, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out.loss)\n",
    "composer_xent.update(out.logits.cpu()[:,256:,:], batch['labels'].cpu()[:,256:])\n",
    "print(composer_xent.compute())\n",
    "\n",
    "# shifting gives us 17.7 --> 0.52\n",
    "# compare all but last logit to labels starting at 1\n",
    "print(torch_xent(out.logits.cpu()[0,:-1,:], batch['labels'].cpu()[0,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([166, 32064])\n",
      "torch.Size([166])\n",
      "tensor(0.5281, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "shift_attention_mask = batch['attention_mask'][..., 1:]\n",
    "logits = out.logits\n",
    "labels = batch['labels']\n",
    "shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n",
    "shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n",
    "\n",
    "print(shift_logits.shape)\n",
    "print(shift_labels.shape)\n",
    "\n",
    "print(torch_xent(shift_logits.cpu(), shift_labels.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  263, 28742, 28713,  4501, 28723,   272,  3469, 28723,     2,     2]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 2204,   263, 28742, 28713,  4501,   297,   272,  3469, 28723,     2]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# print(torch.argmax(shift_logits,dim=-1)[100:])\n",
    "# print(shift_labels[100:])\n",
    "\n",
    "print(torch.argmax(out.logits,dim=-1)[:,-10:])\n",
    "print(batch['labels'][:,-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'The',\n",
       " 'unique',\n",
       " 'characteristic',\n",
       " 'visible',\n",
       " 'on',\n",
       " 'the',\n",
       " 'l',\n",
       " 'amb',\n",
       " 's',\n",
       " \"'\",\n",
       " 'wool',\n",
       " 'is',\n",
       " 'the',\n",
       " 'blue',\n",
       " 'paint',\n",
       " 'mark',\n",
       " 'ings',\n",
       " '.',\n",
       " 'These',\n",
       " 'mark',\n",
       " 'ings',\n",
       " 'are',\n",
       " 'often',\n",
       " 'used',\n",
       " 'by',\n",
       " 'farmers',\n",
       " 'to',\n",
       " 'distinguish',\n",
       " 'and',\n",
       " 'identify',\n",
       " 'the',\n",
       " 'sheep',\n",
       " 'or',\n",
       " 'to',\n",
       " 'indicate',\n",
       " 'certain',\n",
       " 'information',\n",
       " 'about',\n",
       " 'them',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'ownership',\n",
       " 'or',\n",
       " 'management',\n",
       " 'details',\n",
       " '.',\n",
       " 'The',\n",
       " 'presence',\n",
       " 'of',\n",
       " 'these',\n",
       " 'blue',\n",
       " 'paint',\n",
       " 'mark',\n",
       " 'ings',\n",
       " 'on',\n",
       " 'the',\n",
       " 'young',\n",
       " 'sheep',\n",
       " 'lying',\n",
       " 'next',\n",
       " 'to',\n",
       " 'the',\n",
       " 'fence',\n",
       " 'adds',\n",
       " 'an',\n",
       " 'unusual',\n",
       " 'aspect',\n",
       " 'to',\n",
       " 'their',\n",
       " 'appearance',\n",
       " ',',\n",
       " 'which',\n",
       " 'might',\n",
       " 'catch',\n",
       " 'the',\n",
       " 'view',\n",
       " 'er',\n",
       " \"'\",\n",
       " 's',\n",
       " 'attention',\n",
       " 'in',\n",
       " 'the',\n",
       " 'image',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = shift_labels[75:] # batch['labels'] # shift_labels[75:] # \n",
    "labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start',\n",
       " '|',\n",
       " '>',\n",
       " 'blue',\n",
       " 'blue',\n",
       " '|',\n",
       " 'im',\n",
       " '_',\n",
       " 'start',\n",
       " '|',\n",
       " '>',\n",
       " 'system',\n",
       " 'istant',\n",
       " ':',\n",
       " 'blue',\n",
       " 'Q',\n",
       " 'blue',\n",
       " 'characteristic',\n",
       " 'visible',\n",
       " 'on',\n",
       " 'the',\n",
       " 'l',\n",
       " 'amb',\n",
       " 's',\n",
       " \"'\",\n",
       " 'wool',\n",
       " 'is',\n",
       " 'the',\n",
       " 'blue',\n",
       " 'paint',\n",
       " 'mark',\n",
       " 'ings',\n",
       " '.',\n",
       " 'These',\n",
       " 'mark',\n",
       " 'ings',\n",
       " 'are',\n",
       " 'often',\n",
       " 'used',\n",
       " 'by',\n",
       " 'farmers',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'and',\n",
       " 'identify',\n",
       " 'the',\n",
       " 'sheep',\n",
       " ',',\n",
       " 'to',\n",
       " 'indicate',\n",
       " 'certain',\n",
       " 'information',\n",
       " 'about',\n",
       " 'them',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'ownership',\n",
       " 'or',\n",
       " 'management',\n",
       " 'details',\n",
       " '.',\n",
       " 'The',\n",
       " 'blue',\n",
       " 'of',\n",
       " 'these',\n",
       " 'blue',\n",
       " 'paint',\n",
       " 'mark',\n",
       " 'ings',\n",
       " 'on',\n",
       " 'the',\n",
       " 'l',\n",
       " 'sheep',\n",
       " 'lying',\n",
       " 'next',\n",
       " 'to',\n",
       " 'the',\n",
       " 'fence',\n",
       " 'adds',\n",
       " 'an',\n",
       " 'unusual',\n",
       " 'aspect',\n",
       " 'to',\n",
       " 'their',\n",
       " 'appearance',\n",
       " ',',\n",
       " 'as',\n",
       " 'might',\n",
       " 'catch',\n",
       " 'the',\n",
       " 'view',\n",
       " 'er',\n",
       " \"'\",\n",
       " 's',\n",
       " 'attention',\n",
       " '.',\n",
       " 'the',\n",
       " 'image',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.argmax(shift_logits,dim=-1)[65:] # batch['labels'] # shift_labels[75:] # \n",
    "labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The unique characteristic visible on the lambs' wool is the blue paint markings. These markings are often used by farmers to distinguish and identify the sheep or to indicate certain information about them, such as ownership or management details. The presence of these blue paint markings on the young sheep lying next to the fence adds an unusual aspect to their appearance, which might catch the viewer's attention in the image.\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.where(aug_label != -100, aug_label, tokenizer.pad_token_id)\n",
    "tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<|im_start|>system\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible.<|im_end|>\\n<|im_start|>user\\n \\nWhat unique characteristic is visible on the lambs' wool?<|im_end|>\\n<|im_start|>assistant\\n The unique characteristic visible on the lambs' wool is the blue paint markings. These markings are often used by farmers to distinguish and identify the sheep or to indicate certain information about them, such as ownership or management details. The presence of these blue paint markings on the young sheep lying next to the fence adds an unusual aspect to their appearance, which might catch the viewer's attention in the image.\"]\n",
      "[\"<|im_start|>system\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible.<|im_end|>\\n<|im_start|>user\\n \\nWhat unique characteristic is visible on the lambs' wool?<|im_end|>\\n<|im_start|>assistant\\n The unique characteristic visible on the lambs' wool is the blue paint markings. These markings are often used by farmers to distinguish and identify the sheep or to indicate certain information about them, such as ownership or management details. The presence of these blue paint markings on the young sheep lying next to the fence adds an unusual aspect to their appearance, which might catch the viewer's attention in the image.\"]\n",
      "[\"<|im_start|>system\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible.<|im_end|>\\n<|im_start|>user\\n \\nWhat unique characteristic is visible on the lambs' wool?<|im_end|>\\n<|im_start|>assistant\\n The unique characteristic visible on the lambs' wool is the blue paint markings. These markings are often used by farmers to distinguish and identify the sheep or to indicate certain information about them, such as ownership or management details. The presence of these blue paint markings on the young sheep lying next to the fence adds an unusual aspect to their appearance, which might catch the viewer's attention in the image.\"]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(batch):\n",
    "\n",
    "    labels = torch.where(batch['input_ids'] != -100, batch['input_ids'], tokenizer.pad_token_id)\n",
    "    print(tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n",
    "\n",
    "    if i==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "\n",
    "out.loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_labels.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(new_labels.cpu().numpy()[0])#[999:])\n",
    "plt.show()\n",
    "\n",
    "new_labels[0][998:]\n",
    "print(batch['labels'].shape)\n",
    "\n",
    "plt.plot(batch['labels'].cpu().numpy()[0][425:])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(out['logits'].argmax(dim=-1).cpu().numpy()[0][-max_seq_len:])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(batch['labels'].cpu().numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in out:\n",
    "    print(key)\n",
    "print(out['loss'])\n",
    "print('logits shape', out['logits'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in batch:\n",
    "    print(key, batch[key].type())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
